
%%%%%%%%%%%%%%%%%%%%%%
\section{Weighted PCA}
\label{apx:weighted_pca}

Principal components analysis can be thought of as finding a good low-dimensional matrix factorization \citep{engelhardt2010analysis}
that well-approximates the original data in the least-squares sense:
if $C$ is the $N \times N$ genetic covariance matrix,
then to find the top $k$ principal components, 
we find an orthogonal $N \times k$ matrix $U$,
and a $k \times k$ diagonal matrix $\Lambda$ with diagonal entries $\Lambda_{ii}=\lambda_i$ to minimize
\begin{align} \label{eqn:objective}
    \| C - U \Lambda U^T \|^2 = \sum_{ij} \left( C_{ij} - \sum_m \lambda_{m} U_{im} U_{jm} \right)^2 .
\end{align}
The columns of $U$, known as the principal components, are the eigenvectors of $C$,
the entries of $\lambda$ are the eigenvalues of $C$, 
and the proportion of variance explained by the $m^\text{th}$ component is
\begin{align*}
    \frac{ \lambda_m^2 }{ \sum_\ell \lambda_\ell^2 } = \frac{ \sum_{ij} ( \lambda_m U_{im} U_{jm} )^2 }{ \sum_{ij} C_{ij}^2 } .
\end{align*}

Thinking about the problem as a least-squares approximation problem
makes it clear why unbalanced sample sizes can result in undesireable outcomes.
If we want to describe variation \emph{between} populations,
but 80\% of the samples are from a single population,
then unless populations are highly differentiated, 
a better approximation to $C$ may be obtained by using the columns of $U$ to describe variation \emph{within} the overrepresented population
rather than between the populations.
A common workaround is to remove samples,
but a more elegant solution can be found by reweighting the objective function in \eqref{eqn:objective}.
Let $w_{i}$ be a weight associated with sample $i$,
$W$ the diagonal matrix with $w$ along the diagonal,
and instead seek to minimize
\begin{align} \label{eqn:weighted_objective}
    \| W^{1/2} (C - U \Lambda U^T) W^{1/2} \|^2 = \sum_{ij} w_i w_j \left( G_{ij} - \sum_m \lambda_{m} U_{im} U_{jm} \right)^2 ,
\end{align}
and now for convenince we require $U$ to be orthogonal in $\ell_2(w)$, i.e., that $U^T W U =I$.
We then would choose $w$ to give roughly equal weight to each \emph{population},
instead of each individual.
We have used with good results the weightings
$w_i = 1/\max(10,n_i)$,
where $n_i$ is, if there are discrete populations,
the number of samples in the same population as sample $i$;
or, for continuously sampled individuals,
the number of samples within a certain distance of sample $i$.

To solve \eqref{eqn:weighted_objective},
let $\lambda$ and $V$ denote the top $k$ eigenvalues and eigenvectors of $W^{1/2} C W^{1/2}$,
so that $V \Lambda V^T$ is the rank $k$ matrix closest in least squares to $W^{1/2} C W^{1/2}$;
so if we define $U = W^{-1/2} V$
then $U^T W U = V^T V = I$,
and 
\begin{align*}
    W^{-1/2} V \Lambda V^T W^{-1/2} 
    =
    U \Lambda U^T
\end{align*}
is the low-dimensional approximation to $C$.
The proportion of variance explained is calculated from eigenvalues as before,
but has the interpretation
\begin{align*}
    \frac{ \lambda_m^2 }{ \sum_\ell \lambda_\ell^2 } 
    = 
    \frac{ \sum_{ij} w_i w_j ( \lambda_m U_{im} U_{jm} )^2 }{ \sum_{ij} w_i w_j C_{ij}^2 } .
\end{align*}
In our R implementation we use the Spectra library \citep{qiu2016rspectra}
to find only the top $k$ eigenvectors.


